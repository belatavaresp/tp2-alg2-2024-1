{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrição do problema (k-centros)\n",
    "Dado um conjunto de $n$ pontos e um parâmetro $k$, deseja-se encontrar $k$ centros de forma que os pontos estejam o mais próximo possível desses centros (cada ponto está localizado a uma distância máxima $r$ de um dos centros).\n",
    "\n",
    "**Entrada:** Conjunto de pontos $S = {s_1, s_2, ..., s_n}$; Função(métrica) de distância $\\text{dist}: S \\times S \\rightarrow \\mathbb{R}^+$; Inteiro (número de centros) $k$.\n",
    "\n",
    "**Saída:** Conjunto de centros $C = {c_1, c_2, ..., c_k}$\n",
    "\n",
    "**Objetivo:** minimizar o $r$ máximo dos clusters, $r(C) = \\max \\{ \\text{dist}(s_i, C) \\}$ onde $\\text{dist}(s_i, C) = \\min \\{ \\text{dist}(s_i, c_j) \\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliotecas\n",
    "\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets (1min)\n",
    "absenteeism_at_work = fetch_ucirepo(id=445) \n",
    "statlog_vehicle_silhouettes = fetch_ucirepo(id=149) \n",
    "rice_cammeo_and_osmancik = fetch_ucirepo(id=545) \n",
    "raisin = fetch_ucirepo(id=850) \n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "blood_transfusion_service_center = fetch_ucirepo(id=176) \n",
    "mammographic_mass = fetch_ucirepo(id=161) \n",
    "breast_cancer_wisconsin_original = fetch_ucirepo(id=15) \n",
    "maternal_health_risk = fetch_ucirepo(id=863) \n",
    "yeast = fetch_ucirepo(id=110) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the datasets\n",
    "def prepare_data(dataset, label):\n",
    "    X = dataset.data.features\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    y_true = dataset.data.targets\n",
    "    \n",
    "    if hasattr(y_true, 'values'):\n",
    "        y_true = y_true.values.flatten()\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true_encoded = label_encoder.fit_transform(y_true)  # Encode labels as integers\n",
    "    \n",
    "    return X_imputed, y_true_encoded, label\n",
    "\n",
    "real_data = [\n",
    "    prepare_data(absenteeism_at_work, 'abstenteeism_at_work'),\n",
    "    prepare_data(statlog_vehicle_silhouettes, 'statlog_vehicle_silhouettes'),\n",
    "    prepare_data(rice_cammeo_and_osmancik, 'rice_cammeo_and_osmancik'),\n",
    "    prepare_data(raisin, 'raisin'),\n",
    "    prepare_data(wine_quality, 'wine_quality'),\n",
    "    prepare_data(blood_transfusion_service_center, 'blood_transfusion_service_center'),\n",
    "    prepare_data(mammographic_mass, 'mammographic_mass'),\n",
    "    prepare_data(breast_cancer_wisconsin_original, 'breast_cancer_wisconsin_original'),\n",
    "    prepare_data(maternal_health_risk, 'maternal_health_risk'),\n",
    "    prepare_data(yeast, 'yeast')\n",
    "]\n",
    "\n",
    "k_values = [len(np.unique(data[1])) for data in real_data]\n",
    "print(k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Métricas:*** Distância de Minkowski\n",
    "A distância entre dois pontos $x$ e $y$ em $\\mathbb{R}^n$ é dada por:\n",
    "\n",
    "$$\n",
    "\\text{dist}(x, y) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{\\frac{1}{p}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_dist(x,y,p):\n",
    "    # x, y -> tuplas\n",
    "    # p -> int\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    d = 0\n",
    "    \n",
    "    d = np.sum(np.power(np.abs(x - y), p))\n",
    "    \n",
    "    return np.power(d, 1 / p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrizes de distância entre os pontos \n",
    "$p=1$: Distância de Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistMatrix(samp, X, p):\n",
    "    matrix = np.zeros((samp, samp))\n",
    "    for i in range(samp):\n",
    "        for j in range(i + 1, samp):\n",
    "            dist = minkowski_dist(X[i], X[j], p)\n",
    "            matrix[i, j] = dist\n",
    "            matrix[j, i] = dist  # simetria\n",
    "    return matrix\n",
    "\n",
    "X = real_data[0][0]\n",
    "samples = X.shape[0]\n",
    "X_arr = X.to_numpy() if hasattr(X, 'to_numpy') else np.array(X) # convertendo o dataset pra numpy (evitar erro de indexação)\n",
    "manhattan_d = getDistMatrix(samples, X_arr, p=1)\n",
    "print(manhattan_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p=2$: Distância Euclidiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclid_d = getDistMatrix(samples, X_arr, p=2)\n",
    "print(euclid_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrada: número N de samples, número k de centros, matriz dist de distâncias\n",
    "# Saída: raio r, seguido de uma lista com os índices dos k centros escolhidos\n",
    "# A escolha do primeiro centro é aleatória.\n",
    "\n",
    "def alg_maiores_distancias(N, k, dist):\n",
    "    ans = []\n",
    "    ans.append(random.randrange(N))\n",
    "    for i in range(k-1):\n",
    "        best = -math.inf\n",
    "        center = None\n",
    "        for p in range(N):\n",
    "            val = math.inf\n",
    "            for q in ans:\n",
    "                val = min(val, dist[p][q])\n",
    "            if val > best:\n",
    "                center = p\n",
    "                best = val\n",
    "        ans.append(center)\n",
    "\n",
    "    r = -math.inf\n",
    "    for p in range(N):\n",
    "        best = math.inf\n",
    "        for q in ans:\n",
    "            best = min(best, dist[p][q])\n",
    "        r = max(r, best)\n",
    "\n",
    "    return r, ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Entrada: número N de samples, número k de centros, matriz dist de distâncias,\n",
    "# tamanho rmax do intervalo inicial, razão precision do tamanho do intervalo final,\n",
    "# quantidade trials de tentativas para cada iteração da busca binária.\n",
    "#   Saída: raio r, seguido de uma lista com os índices dos k centros escolhidos.\n",
    "#   A cada iteração, a escolha dos centros é aleatória.\n",
    "\n",
    "def alg_busca_binaria(N, k, dist, rmax = None, precision = 0.0001, trials = 100):\n",
    "    if rmax == None:  # O(N^2). É melhor ter isso pré-computado\n",
    "        rmax = 0\n",
    "        for i in range(samples):\n",
    "            for j in range(samples):\n",
    "                rmax = max(rmax, euclid_d[i][j])\n",
    "    lower_bound = 0\n",
    "    upper_bound = rmax\n",
    "    ans = []\n",
    "    while upper_bound - lower_bound >= precision * rmax:\n",
    "        mid = (upper_bound + lower_bound)/2\n",
    "        ok = 0\n",
    "        for _ in range(trials):\n",
    "            centro = [False] * N\n",
    "            coberto = [False] * N\n",
    "            sobra = N\n",
    "            for i in range(k):\n",
    "                c = random.randrange(N)\n",
    "                while centro[c] or (sobra > 0 and coberto[c]):\n",
    "                    c = random.randrange(N)\n",
    "                centro[c] = True\n",
    "                for p in range(N):\n",
    "                    if not coberto[p] and dist[p][c] <= mid:\n",
    "                        coberto[p] = True\n",
    "                        sobra -= 1\n",
    "            if sobra == 0:\n",
    "                ok = 1\n",
    "                upper_bound = mid\n",
    "                ans = []\n",
    "                for i in range(N):\n",
    "                    if centro[i]:\n",
    "                        ans.append(i)\n",
    "                break\n",
    "        if not ok:\n",
    "            lower_bound = mid\n",
    "    return upper_bound, ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados sintéticos\n",
    "### Scikit Learn\n",
    "Snippet de código obtido na referência: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets creation\n",
    "n_samples = 500\n",
    "seed = 30\n",
    "\n",
    "noisy_circles = datasets.make_circles(\n",
    "    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n",
    ")\n",
    "\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\n",
    "\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "no_structure = rng.rand(n_samples, 2), np.zeros(n_samples)\n",
    "\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "data = [\n",
    "    noisy_circles,  \n",
    "    noisy_moons,    \n",
    "    varied,         \n",
    "    aniso,          \n",
    "    blobs,         \n",
    "    no_structure,  \n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"noisy_circles\",\n",
    "    \"noisy_moons\",\n",
    "    \"varied_blobs\",\n",
    "    \"aniso_blobs\",\n",
    "    \"blobs\",\n",
    "    \"no_structure\"\n",
    "]\n",
    "\n",
    "labeled_data = [(X, y, label) for (X, y), label in zip(data, labels)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 9))\n",
    "for i, (X, y) in enumerate(data):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=10, color=\"b\")\n",
    "    plt.title(f\"Dataset {i+1}\")\n",
    "    plt.xlim(-2.5, 2.5)\n",
    "    plt.ylim(-2.5, 2.5)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "\n",
    "def calculate_and_plot_2d_clusters(X, k, ax, title, p):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    n_samples = X.shape[0]\n",
    "    dist_matrix = getDistMatrix(n_samples, X, p)\n",
    "\n",
    "    # alg_maiores_distancias\n",
    "    r_maiores, centers_maiores = alg_maiores_distancias(n_samples, k, dist_matrix)\n",
    "    labels_maiores = np.zeros(n_samples, dtype=int)\n",
    "    for p in range(n_samples):\n",
    "        distances_to_centers = [dist_matrix[p][center] for center in centers_maiores]\n",
    "        labels_maiores[p] = np.argmin(distances_to_centers)\n",
    "\n",
    "    # alg_busca_binaria\n",
    "    r_busca, centers_busca = alg_busca_binaria(n_samples, k, dist_matrix)\n",
    "    labels_busca = np.zeros(n_samples, dtype=int)\n",
    "    for p in range(n_samples):\n",
    "        distances_to_centers = [dist_matrix[p][center] for center in centers_busca]\n",
    "        labels_busca[p] = np.argmin(distances_to_centers)\n",
    "\n",
    "    # Plot    \n",
    "    ax[0].scatter(X[:, 0], X[:, 1], c=[colors[label % len(colors)] for label in labels_maiores], s=10)\n",
    "    ax[0].set_title(f\"{title} - alg_1\")\n",
    "    ax[0].set_xlim(-2.5, 2.5)\n",
    "    ax[0].set_ylim(-2.5, 2.5)\n",
    "    ax[0].set_xticks(())\n",
    "    ax[0].set_yticks(())\n",
    "\n",
    "    ax[1].scatter(X[:, 0], X[:, 1], c=[colors[label % len(colors)] for label in labels_busca], s=10)\n",
    "    ax[1].set_title(f\"{title} - alg_2\")\n",
    "    ax[1].set_xlim(-2.5, 2.5)\n",
    "    ax[1].set_ylim(-2.5, 2.5)\n",
    "    ax[1].set_xticks(())\n",
    "    ax[1].set_yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "k=3\n",
    "for i, (X, y) in enumerate(data):\n",
    "    if X is not None:\n",
    "        ax = plt.subplot(4, 4, 2 * i + 1)\n",
    "        calculate_and_plot_2d_clusters(X, k, [ax, plt.subplot(4, 4, 2 * i + 2)], f\"Dataset {i+1}\", p=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição normal multivariada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(dts=10, cts=3, points_center=100, desv=None):\n",
    "    if desv is None:\n",
    "        desv = [0.1, 0.5, 1.0]  # desvios: baixo, médio e alto\n",
    "    datasets = []\n",
    "\n",
    "    ind = 1\n",
    "    for desvio in desv:\n",
    "        for _ in range(dts):\n",
    "            centers = np.random.rand(cts, 2) * 4 - 2\n",
    "            X = []\n",
    "            y = []\n",
    "            for i, center in enumerate(centers):\n",
    "                points = np.random.multivariate_normal(center, np.eye(2) * desvio, points_center)\n",
    "                X.append(points)\n",
    "                y.append(np.full(points_center, i))\n",
    "            X = np.vstack(X)\n",
    "            y = np.hstack(y)\n",
    "            datasets.append((X, y, f\"{ind}: {desvio}\"))\n",
    "            ind += 1\n",
    "            \n",
    "    return datasets\n",
    "\n",
    "# gerando os conjuntos de dados\n",
    "data_2 = generate_datasets()\n",
    "\n",
    "# Plots\n",
    "def plot_generated_datasets(datasets):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (X, y, title) in enumerate(datasets):\n",
    "        ax = plt.subplot(5, 6, i + 1)\n",
    "        cluster_colors = [colors[label % len(colors)] for label in y]\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=cluster_colors, s=10)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_generated_datasets(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets = len(data_2)\n",
    "n_rows = (n_datasets * 2 + 1) // 6\n",
    "plt.figure(figsize=(15, n_rows*2))\n",
    "\n",
    "for i, (X, y, title) in enumerate(data_2):\n",
    "    ax = plt.subplot(n_rows, 6, 2 * i + 1)\n",
    "    calculate_and_plot_2d_clusters(X, k=3, ax=[ax, plt.subplot(n_rows, 6, 2 * i + 2)], title=title, p=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo o score de silhueta quando a métrica não é euclidiana\n",
    "def silhouette_score_custom(X, labels, distance_matrix):\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_samples = len(labels)\n",
    "    if len(unique_labels) == 1:\n",
    "        return 0.0\n",
    "    \n",
    "    a = np.zeros(n_samples)\n",
    "    for label in unique_labels:\n",
    "        mask = (labels == label)\n",
    "        cluster_points = distance_matrix[np.ix_(mask, mask)]\n",
    "        a[mask] = np.mean(cluster_points, axis=1)\n",
    "    \n",
    "    b = np.full(n_samples, np.inf)\n",
    "    for label in unique_labels:\n",
    "        mask = (labels == label)\n",
    "        other_labels = unique_labels[unique_labels != label]\n",
    "        for other_label in other_labels:\n",
    "            other_mask = (labels == other_label)\n",
    "            other_cluster_points = distance_matrix[np.ix_(mask, other_mask)]\n",
    "            mean_dist = np.mean(other_cluster_points, axis=1)\n",
    "            b[mask] = np.minimum(b[mask], mean_dist)\n",
    "    \n",
    "    silhouette_scores = (b - a) / np.maximum(a, b)\n",
    "    return np.mean(silhouette_scores)\n",
    "\n",
    "# calcular métricas de qualidade\n",
    "def evaluate(X, y_true, labels, metric=\"euclidean\", matrix=None):\n",
    "    if(metric == \"euclidean\"):\n",
    "        silhouette = silhouette_score(X, labels, metric=metric)\n",
    "        rand_index = adjusted_rand_score(y_true, labels)\n",
    "    else:\n",
    "        silhouette = silhouette_score_custom(X, labels, matrix)\n",
    "        rand_index = adjusted_rand_score(y_true, labels)\n",
    "        \n",
    "    return silhouette, rand_index\n",
    "\n",
    "# realizar experimentos com o algoritmo de maiores distâncias\n",
    "def run_maiores_distancias(datasets, k=3, exec=30, k_list=None):\n",
    "    results = []\n",
    "    \n",
    "    if(k_list==None):\n",
    "        # dataset sintético\n",
    "        for X, y_true, label in datasets:\n",
    "            X = StandardScaler().fit_transform(X)\n",
    "            N = len(X)\n",
    "            \n",
    "            for p in [1, 2]:\n",
    "                dist_matrix = getDistMatrix(N, X, p)\n",
    "                distance_type = \"manhattan\" if p == 1 else \"euclidean\"\n",
    "                \n",
    "                for _ in range(exec):\n",
    "                    start_time = time.time()\n",
    "                    r, centers = alg_maiores_distancias(N, k, dist_matrix)\n",
    "                    exec_time = time.time() - start_time\n",
    "                    \n",
    "                    labels = np.argmin(dist_matrix[:, centers], axis=1)\n",
    "                    \n",
    "                    silhouette, rand_index = evaluate(X, y_true, labels, distance_type, dist_matrix)\n",
    "                    \n",
    "                    results.append([label, distance_type, r, 'Maiores Distancias', silhouette, rand_index, exec_time])\n",
    "    else:\n",
    "        #dataset real\n",
    "        for (X, y_true, label), kx in zip(datasets, k_list):\n",
    "            X = StandardScaler().fit_transform(X)\n",
    "            N = len(X)\n",
    "\n",
    "            for p in [1, 2]:\n",
    "                dist_matrix = getDistMatrix(N, X, p)\n",
    "                distance_type = \"manhattan\" if p == 1 else \"euclidean\"\n",
    "                \n",
    "                for _ in range(exec):\n",
    "                    start_time = time.time()\n",
    "                    r, centers = alg_maiores_distancias(N, kx, dist_matrix)\n",
    "                    exec_time = time.time() - start_time\n",
    "                    \n",
    "                    labels = np.argmin(dist_matrix[:, centers], axis=1)\n",
    "                    \n",
    "                    silhouette, rand_index = evaluate(X, y_true, labels, distance_type, dist_matrix)\n",
    "                    \n",
    "                    results.append([label, distance_type, r, 'Maiores Distancias', silhouette, rand_index, exec_time])\n",
    "                    \n",
    "    return np.array(results)\n",
    "\n",
    "# realizar experimentos com o algoritmo de variação do raio\n",
    "def run_busca_binaria(datasets, exec, k_list):\n",
    "    results = []\n",
    "    for (X, y_true, label), kx in zip(datasets, k_list):\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        N = len(X)\n",
    "    \n",
    "        for p in [1, 2]:\n",
    "            dist_matrix = getDistMatrix(N, X, p)\n",
    "            distance_type = \"manhattan\" if p == 1 else \"euclidean\"\n",
    "            rmax = 0\n",
    "            for i in range(N):\n",
    "                for j in range(N):\n",
    "                    rmax = max(rmax, dist_matrix[i][j])\n",
    "\n",
    "            for it in [2, 3, 4, 5, 6, 20]:\n",
    "                prec = 2**(-it)\n",
    "                \n",
    "                # Estrategia 1\n",
    "                for _ in range(exec):\n",
    "                    start_time = time.process_time()\n",
    "                    r, centers = alg_busca_binaria(N, kx, dist_matrix, rmax = rmax, precision = prec, trials = 1)\n",
    "                    exec_time = time.process_time() - start_time\n",
    "                    \n",
    "                    labels = np.argmin(dist_matrix[:, centers], axis=1)\n",
    "                    silhouette, rand_index = evaluate(X, y_true, labels, distance_type, dist_matrix)\n",
    "                    results.append([label, distance_type, r, 'Busca Binaria 1', it, silhouette, rand_index, exec_time])\n",
    "    \n",
    "                # Estrategia 2\n",
    "                start_time = time.process_time()\n",
    "                r, centers = alg_busca_binaria(N, kx, dist_matrix, rmax = rmax, precision = prec, trials = 30)\n",
    "                exec_time = time.process_time() - start_time\n",
    "    \n",
    "                labels = np.argmin(dist_matrix[:, centers], axis=1)\n",
    "                silhouette, rand_index = evaluate(X, y_true, labels, distance_type, dist_matrix)\n",
    "                results.append([label, distance_type, r, 'Busca Binaria 2', it, silhouette, rand_index, exec_time])\n",
    "    return np.array(results)\n",
    "\n",
    "# salvar tabelas\n",
    "def save_csv(results, filename=\"clustering_results.csv\"):\n",
    "    header = [\"Dataset\", \"Distance\", \"Radius\", \"Algorithm\", \"Silhouette Score\", \"Adjusted Rand Index\", \"Execution Time\"]\n",
    "\n",
    "    with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos com o k-means da scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encontrar raio k-means scikit\n",
    "def get_radius(N, centers, labels, dist_matrix):\n",
    "    max_radii = []\n",
    "    \n",
    "    for i in range(len(centers)):\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "        max_distance = -np.inf\n",
    "        for idx in cluster_indices:\n",
    "            for center_idx in range(len(centers)):\n",
    "                if labels[idx] == i:\n",
    "                    max_distance = max(max_distance, dist_matrix[idx][center_idx])\n",
    "        max_radii.append(max_distance)\n",
    "    \n",
    "    return max(max_radii)\n",
    "\n",
    "# realizar experimentos com o algoritmo da scikit\n",
    "def run_kmeans(datasets, k=3, exec=30, k_list=None):\n",
    "    results = []\n",
    "    \n",
    "    if(k_list==None):\n",
    "        #dataset sintético\n",
    "        for X, y_true, label in datasets:\n",
    "            X = StandardScaler().fit_transform(X)\n",
    "            N = len(X)\n",
    "\n",
    "            for p in [1, 2]:\n",
    "                dist_matrix = getDistMatrix(N, X, p)\n",
    "                distance_type = \"manhattan\" if p == 1 else \"euclidean\"\n",
    "                \n",
    "                for _ in range(exec):\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "                    labels = kmeans.labels_\n",
    "                    exec_time = time.time() - start_time\n",
    "                    centers = kmeans.cluster_centers_\n",
    "                    radius = get_radius(N, centers, labels, dist_matrix)\n",
    "\n",
    "                    \n",
    "                    silhouette, rand_index = evaluate(X, y_true, labels, distance_type, dist_matrix)\n",
    "\n",
    "                    results.append([label, distance_type, radius, 'KMeans', silhouette, rand_index, exec_time])\n",
    "    else:\n",
    "        #dataset real\n",
    "        for (X, y_true, label), kx in zip(datasets, k_list):\n",
    "            X = StandardScaler().fit_transform(X)\n",
    "            N = len(X)\n",
    "\n",
    "            for p in [1, 2]:\n",
    "                dist_matrix = getDistMatrix(N, X, p)\n",
    "                distance_type = \"manhattan\" if p == 1 else \"euclidean\"\n",
    "                \n",
    "                for _ in range(exec):\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    kmeans = KMeans(n_clusters=kx, random_state=0).fit(X)\n",
    "                    labels = kmeans.labels_\n",
    "                    exec_time = time.time() - start_time\n",
    "                    centers = kmeans.cluster_centers_\n",
    "                    radius = get_radius(N, centers, labels, dist_matrix)\n",
    "                    silhouette, rand_index = evaluate(X, y_true, labels, distance_type, dist_matrix)\n",
    "\n",
    "                    results.append([label, distance_type, radius, 'KMeans', silhouette, rand_index, exec_time])\n",
    "    \n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_experiment_results(array1, array2, csv=None):\n",
    "    combined_array = np.concatenate([array1, array2])\n",
    "\n",
    "    df = pd.DataFrame(combined_array, columns=['label', 'distance_type', 'radius', 'algorithm', 'silhouette', 'rand_index', 'exec_time'])\n",
    "\n",
    "    df[['silhouette', 'rand_index', 'exec_time']] = df[['silhouette', 'rand_index', 'exec_time']].astype(float)\n",
    "\n",
    "    summary_df = df.groupby(['label', 'distance_type', 'algorithm']).agg(\n",
    "        silhouette_mean=('silhouette', 'mean'),\n",
    "        silhouette_std=('silhouette', 'std'),\n",
    "        rand_index_mean=('rand_index', 'mean'),\n",
    "        rand_index_std=('rand_index', 'std'),\n",
    "        exec_time_mean=('exec_time', 'mean'),\n",
    "        exec_time_std=('exec_time', 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    if csv:\n",
    "        summary_df.to_csv(csv, index=True)\n",
    "        \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos com os dados sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempo de execução médio 3 min\n",
    "\n",
    "results_md_1 = run_maiores_distancias(labeled_data)\n",
    "print(\"Dataset 1: experimentos com algoritmo 1 concluídos\")\n",
    "results_kmeans_1 = run_kmeans(labeled_data)\n",
    "print(\"Dataset 1: experimentos com k-means concluídos\")\n",
    "results_md_2 = run_maiores_distancias(data_2)\n",
    "print(\"Dataset 2: experimentos com algoritmo 1 concluídos\")\n",
    "results_kmeans_2 = run_kmeans(data_2)\n",
    "print(\"Dataset 2: experimentos com k-means concluídos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bb_1 = run_busca_binaria(labeled_data, 30, [3] * len(labeled_data))\n",
    "results_bb_2 = run_busca_binaria(data_2, 30, [3] * len(data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_bsearch_results(results):\n",
    "    df = pd.DataFrame(results, columns=['label', 'distance_type', 'radius', 'algorithm', 'precision', 'silhouette', 'rand_index', 'exec_time'])\n",
    "    \n",
    "    df[['radius', 'silhouette', 'rand_index', 'exec_time']] = df[['radius', 'silhouette', 'rand_index', 'exec_time']].astype(float)\n",
    "    \n",
    "    summary_df = df.groupby(['label', 'distance_type', 'algorithm', 'precision']).agg(\n",
    "        radius_max=('radius', 'max'),\n",
    "        silhouette_mean=('silhouette', 'mean'),\n",
    "        #silhouette_std=('silhouette', 'std'),\n",
    "        silhouette_max=('silhouette', 'max'),\n",
    "        rand_index_mean=('rand_index', 'mean'),\n",
    "        #rand_index_std=('rand_index', 'std'),\n",
    "        rand_index_max=('rand_index', 'max'),\n",
    "        exec_time_mean=('exec_time', 'mean'),\n",
    "        #exec_time_std=('exec_time', 'std'),\n",
    "    ).reset_index()\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "aggregate_bsearch_results(results_bb_1).to_csv('resultado_bsearch_sintetico_1.csv')\n",
    "aggregate_bsearch_results(results_bb_2).to_csv('resultado_bsearch_sintetico_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table_1 = aggregate_experiment_results(results_md_1, results_kmeans_1, csv='resultado_1.csv')\n",
    "summary_2 = aggregate_experiment_results(results_md_2, results_kmeans_2)\n",
    "summary_2['label'] = summary_2['label'].astype(str)\n",
    "summary_2['label_first_number'] = summary_2['label'].str.extract(r'(\\d+)').astype(int)\n",
    "summary_table_2 = summary_2.sort_values(by='label_first_number').drop(columns=['label_first_number'])\n",
    "summary_table_2.to_csv('resultado_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos com os dados reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_real = run_maiores_distancias(real_data, 3, 30, k_values)\n",
    "print(\"Dataset real: experimentos com algoritmo 1 concluídos\")\n",
    "save_csv(results_real, filename='cluster_real_1.csv')\n",
    "\n",
    "results_kmeans_real = run_kmeans(real_data, 3, 30, k_values)\n",
    "save_csv(results_kmeans_real, filename='k_means_real.csv')\n",
    "print(\"Dataset real: experimentos com k-means concluídos\")\n",
    "\n",
    "# Demora uns 10 min\n",
    "results_bsearch_real = run_busca_binaria(real_data, 30, k_values)\n",
    "print(\"Dataset real: experimentos com algoritmo 2 concluídos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(results_bsearch_real, filename='bsearch_real_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_real = aggregate_experiment_results(results_real, results_kmeans_real, csv='resultado_real_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_bsearch_results(results_bsearch_real).to_csv('resultado_bsearch_real_1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
